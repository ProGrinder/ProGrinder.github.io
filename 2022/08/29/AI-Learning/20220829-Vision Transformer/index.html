<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/default/greycat.ico"><link rel="icon" type="image/png" href="/img/default/greycat.ico"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content=""><meta name="author" content="Grinder"><meta name="keywords" content=""><title>Vision Transformer - Be Grinder,for Better</title><link rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/prism/1.21.0/themes/prism-coy.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_pf9vaxs7x7b.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1746382_qrhzb41budd.css"><link rel="stylesheet" href="/css/Normalize.css"><link rel="stylesheet" href="/css/GrinderStyle.css"><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.2.0"></head><body><header style="height:72vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>Grinder's Space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item"><a class="nav-link" href="/links/"><i class="iconfont icon-link-fill"></i> 友链</a></li><li class="nav-item" id="search-btn"><a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" href="javascript:">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner intro-2" id="background" parallax="true" style="background:url(/img/default/None.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.07)"><div class="container page-header text-center fade-in-up"><span class="h2" id="subtitle"></span><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2022-08-29 17:00" pubdate>2022年8月29日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 967 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 16 分钟 </span><span id="busuanzi_container_page_pv" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="busuanzi_value_page_pv"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid"><div class="row"><div class="d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-md"><div class="container nopadding-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto" id="post"><h1 style="display:none">Vision Transformer</h1><div class="markdown-body" id="post-body"><h1 id="vision-transformer"><a class="markdownIt-Anchor" href="#vision-transformer"></a> Vision Transformer</h1><h2 id="vision-transformer-2"><a class="markdownIt-Anchor" href="#vision-transformer-2"></a> Vision Transformer</h2><p><strong>论文题目</strong> ：AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</p><p><strong>论文地址</strong> ：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.11929.pdf">https://arxiv.org/pdf/2010.11929.pdf</a></p><p><strong>论文出处</strong> ：ICLR’2021</p><p><strong>代码实现</strong> ：-</p><h3 id="idea"><a class="markdownIt-Anchor" href="#idea"></a> Idea</h3><p>通过将图像拆分为块 (patch)，并将这些图像块的线性嵌入序列作为 Transformer 的输入，作者将Transformer成功应用到CV领域。</p><blockquote><p>没有直接把像素点作为输入，可以减少参数量</p></blockquote><p>通过这篇文章的实验，说明Transformer在CV领域确实有效，并且效果惊人。</p><h3 id="detail"><a class="markdownIt-Anchor" href="#detail"></a> Detail</h3><h4 id="network"><a class="markdownIt-Anchor" href="#network"></a> Network</h4><p><img src="/img/article/2022/08/29/2.png" srcset="/img/loading.gif" alt=""></p><ul><li>Linear Projection of Flattened Patches(Patch Embedding层)</li><li>Transformer Encoder(图右侧有给出更加详细的结构)</li><li>MLP Head（最终用于分类的层结构）</li></ul><h4 id="patch-embedding"><a class="markdownIt-Anchor" href="#patch-embedding"></a> Patch Embedding</h4><p><img src="/img/article/2022/08/29/3.png" srcset="/img/loading.gif" alt=""></p><p>以ViT-Base/16为例:</p><p>首先将一张图片按给定大小分成一堆Patches。将输入图片(224x224)按照16x16大小的Patch进行划分，划分后会得到(224/16)^2 = 196 个patch。</p><p>每个Patche数据shape为<code>[16,16,3]</code>通过线性映射将每个Patch映射到一维向量中，映射得到一个长度为768的向量token<code>[196,768]</code>。</p><blockquote><p>具体使用一个卷积层（768个16x16的卷积核,stride为16）来实现Patch划分。</p></blockquote><p>拼接一个[class]token， Concat([1,768],[196,768] =&gt; <code>[197,768]</code>。</p><p>叠加位置编码Position Embedding, <code>[197,768]</code> =&gt; <code>[197,768]</code>。</p><blockquote><p>自注意力的<strong>扰动不变性</strong>(<strong>Permutation-invariant</strong>): <strong>打乱 Sequence 中 tokens 的顺序并不会改变结果</strong>。</p><p>这里的[class] token和位置编码都是可训练参数。</p></blockquote><p><img src="/img/article/2022/08/29/4.png" srcset="/img/loading.gif" alt=""></p><p>位置编码之间的余弦相似度可视化，表明任意两个patches之间在位置上的关联度。</p><p>可以发现相近的图像块的位置编码关联度较高，且同行或列的位置编码关联度也相近。</p><p>这里的图片按32x32的大小划分，所以是得到7x7个patch。</p><h4 id="transformer-encoder"><a class="markdownIt-Anchor" href="#transformer-encoder"></a> Transformer Encoder</h4><p><img src="/img/article/2022/08/29/5.png" srcset="/img/loading.gif" alt=""></p><p>Transformer Encoder其实就是重复堆叠Encoder Block L次:</p><p><img src="/img/article/2022/08/29/6.png" srcset="/img/loading.gif" alt=""></p><ul><li><p>Layer Norm，这种Normalization方法主要是针对NLP领域提出的，这里是对每个token进行Norm处理</p><blockquote><p>BN是取不同样本的同一个通道的特征做归一化；LN则是取同一个样本的不同通道做归一化。</p></blockquote></li><li><p>Multi-Head Attention，这个结构之前是在Attention Is All You Need这篇文章中提出</p><p><img src="/img/article/2022/08/29/7.png" srcset="/img/loading.gif" alt=""></p><p><img src="/img/article/2022/08/29/8.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>我感觉这原文当中这两个图比较抽象，大概讲下我的理解:</p><p><strong>Self-Attention</strong> :</p><p><img src="/img/article/2022/08/29/9.png" srcset="/img/loading.gif" alt=""></p><p>首先把输入的token通过Input Embedding映射为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">a_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>，然后<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">a_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>通过3个可变换矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>q</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>k</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">W_q,W_k,W_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.969438em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:-.13889em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.03588em">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-.13889em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:-.13889em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>(这三个参数是可训练的，并且是共享的)计算得到对应的Q,K,V。</p><ul><li>q代表query，后续会去和每一个k进行匹配</li><li>k代表key，后续会被每个q匹配</li><li>v代表value，从a中提取得到的信息，可以理解为相关性权值</li></ul><p>Q和K相乘，得到是查询向量和各个对应的键向量的相关性（匹配度），是 n×n 的矩阵。</p><p>除以根号dk，再通过SoftMax得到缩放后的attention score，再与V相乘，得到加权和作为最后的输出。</p><p>点乘操作可以写成矩阵乘法,实现计算并行化，因此计算速度会快很多。</p><p><strong>Multi-Head Attention</strong> :</p><p>Q，V，K分别通过n次线性变换得到n组Q，K，V，这里n对应着<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi><mi>e</mi><mi>a</mi><msub><mi>d</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">Head_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord mathdefault" style="margin-right:.08125em">H</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></p><p>对于每一组<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>Q</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>K</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>V</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">Q_i,K_i,V_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8777699999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.07153em">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.07153em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.22222em">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.22222em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>通过Self-Attention得到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi><mi>e</mi><mi>a</mi><msub><mi>d</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">Head_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord mathdefault" style="margin-right:.08125em">H</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></p><p>拼接所有的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi><mi>e</mi><mi>a</mi><msub><mi>d</mi><mn>0</mn></msub><mo>−</mo><mi>H</mi><mi>e</mi><mi>a</mi><msub><mi>d</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">Head_0 - Head_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord mathdefault" style="margin-right:.08125em">H</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord mathdefault" style="margin-right:.08125em">H</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>，然后将其线性映射得到最终输出。</p></blockquote></li><li><p>Dropout，减少过拟合，保证模型的稀疏性</p></li><li><p>MLP，如图所示，就是全连接+GELU激活函数+Dropout</p></li></ul><h4 id="mlp-head"><a class="markdownIt-Anchor" href="#mlp-head"></a> MLP Head</h4><p><img src="/img/article/2022/08/29/10.png" srcset="/img/loading.gif" alt=""></p><p>上面通过Transformer Encoder后输出的shape和输入的shape是保持不变的。</p><p>所以我们只需要提取出[class]token生成的对应结果就行，即<code>[197, 768]</code>中提取出[class]token对应的<code>[1, 768]</code>。</p><p>接着我们通过MLP Head得到我们最终的分类结果。</p><p>MLP Head 由 Linear+tanh激活函数+Linear组成。</p><p>整体网络的内部结构:</p><p><img src="/img/article/2022/08/29/11.png" srcset="/img/loading.gif" alt=""></p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a> <a class="hover-with-bg" href="/tags/%E6%97%8B%E8%BD%AC%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">旋转目标检测</a> <a class="hover-with-bg" href="/tags/Transformer/">Transformer</a></div></div><p class="note note-warning">本博客所有文章均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-NC-SA 4.0 协议</a> ，禁止商用，转载请注明出处！</p><div class="post-prevnext row"><article class="post-prev col-6"><a href="/2022/09/06/Seminar/20220906-%E7%BB%84%E4%BC%9A/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">2022-09-06-组会</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2022/08/22/Seminar/20220822-%E7%BB%84%E4%BC%9A/"><span class="hidden-mobile">2022-08-22-组会</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments"><div id="vcomments"></div><script type="text/javascript">function loadValine(){addScript("https://cdn.staticfile.org/valine/1.4.14/Valine.min.js",(function(){new Valine({el:"#vcomments",app_id:"zFsXUmmjrQgI8Mwc4q1hoeFX-gzGzoHsz",app_key:"lhfQutIrdSGngahzOrpkaKBU",placeholder:"说点什么吧！~(支持Markdown语法)",path:window.location.pathname,avatar:"retro",meta:["nick","mail","link"],pageSize:"10",lang:"zh-CN",highlight:!0,recordIP:!1,serverURLs:""})}))}waitElementVisible("vcomments",loadValine)</script><noscript>Please enable JavaScript to view the <a target="_blank" href="https://valine.js.org" rel="nofollow noopener noopener">comments powered by Valine.</a></noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div id="tocbot"></div></div></div></div></div></main><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div><div id="aplayer"></div><script defer src="https://cdn.staticfile.org/aplayer/1.10.1/APlayer.min.js"></script><link rel="stylesheet" href="https://cdn.staticfile.org/aplayer/1.10.1/APlayer.min.css"><script type="text/javascript">var oldLoadAp=window.onload;window.onload=function(){oldLoadAp&&oldLoadAp(),new APlayer({container:document.getElementById("aplayer"),fixed:!0,autoplay:!1,loop:"all",order:"list",theme:"#b7daff",preload:"auto",lrcType:3,audio:[{name:"Reforget",artist:"Lauv",url:"/songs/Reforget.wav",cover:"/img/songs/Cover_Reforget.jpg",lrc:"/songs/lrc/Reforget.lrc"},{name:"Saint",artist:"Travis Scott & Quavo",url:"/songs/Saint.wav",cover:"/img/songs/Cover_Saint.jpg",lrc:"/songs/lrc/Saint.lrc"},{name:"Antisocial",artist:"Travis Scott & Ed Sheeran",url:"/songs/Antisocial.wav",cover:"/img/songs/Cover_Antisocial.jpg",lrc:"/songs/lrc/Antisocial.lrc"},{name:"Champion",artist:"Travis Scott & Nav",url:"/songs/Champion.wav",cover:"/img/songs/Cover_Champion.jpg",lrc:"/songs/lrc/Champion.lrc"},{name:"Smile",artist:"The Weekend & Juice WRLD",url:"/songs/Smile.wav",cover:"/img/songs/Cover_Smile.jpg",lrc:"/songs/lrc/Smile.lrc"},{name:"Starboy",artist:"The Weekend",url:"/songs/Starboy.wav",cover:"/img/songs/Cover_Starboy.jpg",lrc:"/songs/lrc/Starboy.lrc"},{name:"Sidewalks",artist:"The Weekend",url:"/songs/Sidewalks.wav",cover:"/img/songs/Cover_Sidewalks.jpg",lrc:"/songs/lrc/Sidewalks.lrc"},{name:"Reminder",artist:"The Weekend",url:"/songs/Reminder.wav",cover:"/img/songs/Cover_Reminder.jpg",lrc:"/songs/lrc/Reminder.lrc"},{name:"Die For You",artist:"The Weekend",url:"/songs/Die For You.wav",cover:"/img/songs/Cover_Die For You.jpg",lrc:"/songs/lrc/Die For You.lrc"},{name:"Pray For Me",artist:"The Weekend & Kendrick Lamar",url:"/songs/Pray For Me.wav",cover:"/img/songs/Cover_Pray For Me.jpg",lrc:"/songs/lrc/Pray For Me.lrc"}]})}</script><footer class="mt-5"><div class="text-center py-3"><div><br>Powered by: <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> Theme: <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div style="font-size:.85rem"><span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div></div></footer><script src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js"></script><script src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js"></script><script src="/js/debouncer.js"></script><script src="/js/main.js"></script><script src="/js/lazyload.js"></script><script src="https://cdn.staticfile.org/prism/1.21.0/components/prism-core.min.js"></script><script src="https://cdn.staticfile.org/prism/1.21.0/plugins/autoloader/prism-autoloader.min.js"></script><script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js"></script><script src="/js/clipboard-use.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js"></script><script>$(document).ready((function(){var t=$("#board-ctn").offset().top;tocbot.init({tocSelector:"#tocbot",contentSelector:"#post-body",headingSelector:"h1,h2,h3,h4,h5,h6",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",collapseDepth:0,scrollSmooth:!0,headingsOffset:-t}),$(".toc-list-item").length>0&&$("#toc").css("visibility","visible")}))</script><script src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js"></script><script>var typed=new Typed("#subtitle",{strings:["  ","Vision Transformer&nbsp;"],cursorChar:"_",typeSpeed:72,loop:!1});typed.stop(),$(document).ready((function(){$(".typed-cursor").addClass("h2"),typed.start()}))</script><script src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js"></script><script>anchors.options={placement:"right",visible:"hover"};var el="h1,h2,h3,h4,h5,h6".split(","),res=[];for(item of el)res.push(".markdown-body > "+item);anchors.add(res.join(", "))</script><script src="/js/local-search.js"></script><script>var path="/local-search.xml",inputArea=document.querySelector("#local-search-input");inputArea.onclick=function(){searchFunc(path,"local-search-input","local-search-result"),this.onclick=null}</script><script src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js"></script><link rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css"><script>$("#post img:not(.no-zoom img, img[no-zoom]), img[zoom]").each((function(){var t=document.createElement("a");$(t).attr("data-fancybox","images"),$(t).attr("href",$(this).attr("src")),$(this).wrap(t)}))</script><link rel="stylesheet" href="https://cdn.staticfile.org/KaTeX/0.11.1/katex.min.css"><script defer>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?f9aad97d2fdeed725d5d52997eaebaa9";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script></body></html>